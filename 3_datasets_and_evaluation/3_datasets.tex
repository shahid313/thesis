\section{Datasets}

Labeled data has a crucial influence on algorithm development and evaluation inresearch fields dealing with classification and detection. Any machine learning algorithm is only as good as the data behind it in terms of modeling and generalization
properties.

Well-established and easily accessible benchmark databases attract theinterest of the research community as readily available support for research, thus accelerating the pace of development for related fields. There are many well-knowndatabases in related research areas, such as TIMIT \cite{garofolo1993darpa} for speechrecognition or the Million Song Dataset \cite{bertin2011million} for various tasks inmusic information retrieval. In view of this critical influence, the process of creatinga dataset for system developing is, naturally, very delicate. The content must be carefullyselected to provide sufficient coverage of the aspects of interest, sufficient variabilityin characterizing these aspects, and a sufficient quantity of examples for robustmodeling. Unfortunately, there is no rule on what ``sufficient'' means, as it usuallydepends on the projected use of the data.


Compared to speech, the categories and sequences of sound events are not so straightforward to define, as any object or beingmay produce a sound. Sounds can be organized into categories based on differentproperties, such as the sound source (e.g., cup), its production mechanism (e.g.,hit), or even the source properties (e.g., metal), and the same sound can belong tomany different categories, depending on the chosen property (sound when hittinga metallic cup). In addition, there are no specific rules on how environmentalsounds co-occur. 

For this reason, building a database for environmental soundsis a complicated task, with the choice of classes usually dictated by the targetedresearch, and the size limited by practical aspects of data collection especially compared to well-known datasets available for image processing (i.e., Imagenet\cite{deng2009imagenet}). Anyway, very recently a project promoted by Google has been presented \cite{gemmeke2017audio}, with the aim to collect a large human annotated dataset and a relative ontology obtained from YouTube videos.

\subsection{Data Augmentation}
Data augmentation refers to methods for increasing the amount of development dataavailable without additional recordings. With only a small amount of data, systemsoften end up overfitting the training data and performing poorly on unseen data.
Thus, it is desirable to artificially increase the amount and diversity of data used intraining. 

Many techniques have been proposed for data augmentation in domains different from audio, Some approaches extends the augmentation in the feature space [3,7,20] to detect events by processing multimedia stream even if no augmentation of existing data-set in the input space is performed [6,11,12,13]. Events could be used to increase the dimensionality of input space by using virtual sensor(s) generating data stream according to the detected event also adding “virtual examples” [10] also using filters/operators [4]. Dataset augmentation could be used to reduce overfitting while training supervised learning models or to counteract the dataset imbalance i.e., if the classes are not approximately equally represented as in the case of SMOTE [14,15]. In the audio field DA exploits techniques such as pitch shifting, time stretching or the generation of multi-microphone data by means of simulated impulse response of the acoustic space [16-19] or performing the transformation not in input space, but in the feature space [20-21]. With the development of the artificial neural networks (ANN), some works act this augmentation by means of auto-encoder models

16,22] also using Generative ANN [23-24]. The above mentioned approaches have a large impact in the development of data driven approaches to activity recognition, with applications in the Active and Assisted Living (AAL) domain that has a lack of large scale, high quality, and annotated datasets even if open datasets are growing [25]. Crowdsignals.io and the UbiHealth [26] support the definition of protocols for the collection of data, in addition to the collection and annotation of sharable datasets. The EU project OPPORTUNITY is a platform whereby researchers working in different organizations could have access to a common dataset acting as a benchmark [27]. 
A limited number of repositories have supported the notion of shared datasets, including a small number of activity recognition related resources [28]. To address this lack it is possible to simulate smart environments equipped with heterogeneous sensors. However, the results obtained by testing activity recognition algorithms on these synthetic datasets are still far from ideal [29]

\subsection{Evaluation Setup}
During system development, iterative training and testing of the system is necessary
for tuning its parameters. For this purpose, the labeled data available for development is split into disjoint training and testing sets. However, labeled data is often
in short supply, and it is difficult to choose between using more data for training
(leading to better-performing systems) or for testing (giving more precise and
reliable estimates of system performance). For an efficient use of all the available
data, system development can use cross-validation folds [11, p. 483] to artificially
create multiple training/testing splits using the same data; overall performance is
then the average of performance on each split. Cross-validation folds also help avoid
overfitting and supports generalization properties of the system, so that when the
system is used on new data it is expected to have similar performance as seen with
the data used for development.
